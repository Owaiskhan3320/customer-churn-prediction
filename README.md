# Comparative Evaluation of Machine Learning Algorithms for Customer Churn Prediction

##  Overview
This project aims to compare multiple machine learning algorithms to predict customer churn based on historical and behavioral data. By analyzing which customers are likely to leave a service, organizations can take proactive steps to retain them and reduce revenue loss.

---

##  Business Problem Statement
Customer churn poses a critical challenge to businesses, particularly in competitive industries like telecommunications, banking, and retail. Acquiring new customers is often more expensive than retaining existing ones. Hence, identifying customers at high risk of churn enables companies to implement targeted retention strategies and improve profitability.

---

##  Objectives
The main objectives of this project are:

1. To build predictive models that classify whether a customer will churn or not, based on historical and behavioral data.  
2. To evaluate and compare the performance of various machine learning algorithms using metrics such as **accuracy**, **precision**, **recall**, **F1-score**, **ROC AUC**, **specificity**, **balanced accuracy**, **Matthews Correlation Coefficient (MCC)**, and **training time**.  
3. To identify which algorithms are best suited for churn prediction in terms of predictive power and computational feasibility.

---

##  Dataset
- **Source:** [Kaggle](https://www.kaggle.com/)  
- **Description:** The dataset contains customer demographic and service-related information, such as usage patterns, account tenure, and service plans.  
- **Target Variable:** `Churn` (Binary â€” Yes/No)

---

##  Project Workflow
1. **Data Preprocessing**  
   - Handling missing values  
   - Encoding categorical variables  
   - Feature scaling  
   - Splitting data into training and testing sets  

2. **Model Training and Evaluation**  
   - Train multiple ML models  
   - Evaluate using classification metrics  
   - Compare results using accuracy, F1-score, ROC AUC, etc.  

3. **Result Analysis**  
   - Identify top-performing algorithms  
   - Discuss trade-offs between performance and computation time  

---


## Results Summary
After evaluating multiple algorithms, the results indicate that **ensemble-based models** (such as Random Forest and XGBoost) generally outperform simpler models like Logistic Regression in terms of predictive power and stability, while maintaining reasonable computation time.

---

##  Contact
**Author:** Owais Khan  
**Field:** Data Science  
**Email:** (owaiskhan3320@gmail.com)  

